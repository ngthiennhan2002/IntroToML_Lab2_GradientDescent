{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d396d34b",
   "metadata": {
    "colab_type": "text",
    "id": "v6pSj40J774r"
   },
   "source": [
    "# Lab02: Gradient Descent.\n",
    "\n",
    "- Student ID: 20127265\n",
    "- Student name: Nguyễn Thiện Nhân"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c615d12c",
   "metadata": {
    "colab_type": "text",
    "id": "A_TxwHT3774s"
   },
   "source": [
    "**How to do your homework**\n",
    "\n",
    "\n",
    "You will work directly on this notebook; the word `TODO` indicate the parts you need to do.\n",
    "\n",
    "You can discuss ideas with classmates as well as finding information from the internet, book, etc...; but *this homework must be your*.\n",
    "\n",
    "**How to submit your homework**\n",
    "\n",
    "Before submitting, rerun the notebook (`Kernel` ->` Restart & Run All`).\n",
    "\n",
    "Then create a folder named `ID` (for example, if your ID is 1234567, then name the folder `1234567`) Copy file notebook to this folder, compress and submit it on moodle.\n",
    "\n",
    "**Contents:**\n",
    "- Gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e88e6d",
   "metadata": {
    "colab_type": "text",
    "id": "Idy5YdzJ774t"
   },
   "source": [
    "## 1. Loss landscape\n",
    "![Loss lanscape](img.png) <center>**Figure 1. Loss landscape visualized as a 2D plot. Source: codecamp.vn**<center>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; The gradient descent method is an iterative optimization algorithm that operates over a loss landscape (also called an optimization surface).As we can see, our loss landscape has many peaks and valleys based on which values our parameters take on. Each peak is a local maximum that represents very high regions of loss – the local maximum with the largest loss across the entire loss landscape is the global maximum. Similarly, we also have local minimum which represents many small regions of loss. The local minimum with the smallest loss across the loss landscape is our global minimum. In an ideal world, we would like to find this global minimum, ensuring our parameters take on the most optimal possible values.\n",
    " \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Each position along the surface of the corresponds to a particular loss value given a set of\n",
    "parameters $\\mathbf{W}$ (weight matrix) and $\\mathbf{b}$ (bias vector). Our goal is to try different values of $\\mathbf{W}$ and $\\mathbf{b}$, evaluate their loss, and then take a step towards more optimal values that (ideally) have lower loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298275a3",
   "metadata": {
    "colab_type": "text",
    "id": "1PqdcOh4774u"
   },
   "source": [
    "## 2. The “Gradient” in Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32178a01",
   "metadata": {
    "colab_type": "text",
    "id": "oo700mD-774v"
   },
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;We can use $\\mathbf{W}$ and $\\mathbf{b}$ and to compute a loss function $L$ or we are able to find our relative position on the loss landscape, but **which direction** we should take a step to move closer to the minimum.\n",
    "\n",
    "- All We need to do is follow the slope of the gradient $\\nabla_{\\mathbf{W}}$. We can compute the gradient $\\nabla_{\\mathbf{W}}$ across all dimensions using the following equation:\n",
    "$$\\dfrac{df\\left(x\\right)}{dx}=\\lim_{h\\to0} \\dfrac{f\\left(x+h\\right)-f\\left(x\\right)}{h}$$\n",
    "- But, this equation has 2 problems:\n",
    "    + 1. It’s an *approximation* to the gradient.\n",
    "    + 2. It’s painfully slow.\n",
    "    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; In practice, we use the **analytic gradient** instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bc84bd",
   "metadata": {},
   "source": [
    "## 3. Forward & Backward\n",
    "\n",
    "In this section, you will be asked to fill in the black to form the forward process and backward process with the data defined as follows:\n",
    "\n",
    "- Feature: $X$ (shape: $n\\times d$, be already used bias trick)\n",
    "- Label: $y$ (shape: $n\\times 1$)\n",
    "- Weight: $W$ (shape: $d\\times 1$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18584246",
   "metadata": {},
   "source": [
    "### 3.1. Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d025a47c",
   "metadata": {},
   "source": [
    "**TODO**: Fill in the blank\n",
    "\n",
    "$$\n",
    "h = XW \\Rightarrow \\frac{\\partial h}{\\partial W} = X\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma(h) \\Rightarrow \\frac{\\partial \\hat{y}}{\\partial h} = \\sigma(X)\n",
    "$$\n",
    "\n",
    "$$\n",
    "Loss = \\frac{1}{2}(\\hat{y} - y)^2 \\Rightarrow \\frac{\\partial Loss}{\\partial \\hat{y}} = \\hat{y} - y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbf82bb",
   "metadata": {},
   "source": [
    "### 3.2. Backward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c904500c",
   "metadata": {},
   "source": [
    "**Goal**: Compute $\\nabla Loss = \\frac{\\partial Loss(W)}{\\partial W}$\n",
    "\n",
    "**How to compute $\\nabla Loss$?**: Use Chain-rule. Your work is to fill in the blank\n",
    "\n",
    "**TODO**: Fill in the blank\n",
    "$$\n",
    "\\nabla Loss = \\frac{\\partial Loss(W)}{\\partial W} = \\frac{\\partial Loss}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial h} \\cdot \\frac{\\partial h}{\\partial W} = X \\cdot \\sigma(X) \\cdot (\\hat{y} - y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a1908f",
   "metadata": {
    "colab_type": "text",
    "id": "bF9JcwLP774w"
   },
   "source": [
    "## 4. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86896b2",
   "metadata": {
    "colab_type": "text",
    "id": "1xS5CesP774x"
   },
   "source": [
    "### 4.1. Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f2d6f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLJ, DataFrames, VegaLite, Distributions\n",
    "\n",
    "# import your libraries if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636d6be2",
   "metadata": {
    "colab_type": "text",
    "id": "1xS5CesP774x"
   },
   "source": [
    "### 4.2. Create data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "926495fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: make_blobs not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: make_blobs not defined\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ c:\\Users\\pc\\Downloads\\[Julia] Lab02 - GradientDescent.ipynb:2"
     ]
    }
   ],
   "source": [
    "# generate a 2-class classification problem with 1,000 data points, each data point is a 2D feature vector\n",
    "X, y = make_blobs(1000, 2, centers=2, cluster_std=0.5, rng=1)\n",
    "df = DataFrame(X)\n",
    "df.y = convert(Vector{Float64}, y) .- 1\n",
    "\n",
    "# insert a column of 1’s as the last entry in the feature matrix  \n",
    "# -- allows us to treat the bias as a trainable parameter\n",
    "df.x3 = ones(size(df)[1],)\n",
    "println(first(df, 5))\n",
    "\n",
    "# Split data, use 50% of the data for training and the remaining 50% for testing\n",
    "df_train, df_test = partition(df, 0.5)\n",
    "println(size(df_train), size(df_test))\n",
    "X_train, y_train = [df_train.x1 df_train.x2 df_train.x3], df_train.y\n",
    "X_test, y_test = [df_test.x1 df_test.x2 df_test.x3], df_test.y;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ae79fb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "LoadError: UndefVarError: @vlplot not defined\nin expression starting at c:\\Users\\pc\\Downloads\\[Julia] Lab02 - GradientDescent.ipynb:1",
     "output_type": "error",
     "traceback": [
      "LoadError: UndefVarError: @vlplot not defined\n",
      "in expression starting at c:\\Users\\pc\\Downloads\\[Julia] Lab02 - GradientDescent.ipynb:1\n"
     ]
    }
   ],
   "source": [
    "df |> @vlplot(\n",
    "    :point, \n",
    "    x=:x1, y=:x2, \n",
    "    color = :\"y:n\",\n",
    "    width=400,height=400\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a806f7c",
   "metadata": {},
   "source": [
    "### 4.3. Training\n",
    "#### Sigmoid function and derivative of the sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bf0f5f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sigmoid_deriv (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function sigmoid_activation(x)\n",
    "    #TODO\n",
    "    \"\"\"compute the sigmoid activation value for a given input\"\"\"\n",
    "    #return?\n",
    "\n",
    "end\n",
    "\n",
    "function sigmoid_deriv(x)\n",
    "    #TODO\n",
    "    \"\"\"\n",
    "    Compute the derivative of the sigmoid function ASSUMING\n",
    "    that the input 'x' has already been passed through the sigmoid\n",
    "    activation function\n",
    "    \"\"\"\n",
    "    #return?\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3afb0ab",
   "metadata": {},
   "source": [
    "#### Compute output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7c499e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function compute_h(W, X)\n",
    "    #TODO\n",
    "    \"\"\"\n",
    "    Compute output: Take the inner product between our features 'X' and the weight\n",
    "    matrix 'W', then pass this value through our sigmoid activation function \n",
    "    \"\"\"\n",
    "    # return?\n",
    "\n",
    "end\n",
    "\n",
    "function predict(W, X)\n",
    "    #TODO\n",
    "    \"\"\"\n",
    "    Take the inner product between our features and weight matrix, \n",
    "    then pass this value through our sigmoid activation\n",
    "    \"\"\"\n",
    "    # preds = ...\n",
    "\n",
    "    # apply a step function to threshold the outputs to binary\n",
    "    # class labels\n",
    "    preds[preds .<= 0.5] .= 0\n",
    "    preds[preds .> 0] .= 1\n",
    "\n",
    "    return preds\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb37213",
   "metadata": {},
   "source": [
    "#### Compute gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "848a2059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "compute_gradient (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function compute_gradient(error, y_hat, trainX)\n",
    "    #TODO \n",
    "    \"\"\"\n",
    "    the gradient descent update is the dot product between our\n",
    "    features and the error of the sigmoid derivative of\n",
    "    our predictions\n",
    "    \"\"\"\n",
    "    # return?\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cf5d4f",
   "metadata": {},
   "source": [
    "#### Training function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dce6b15d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function train(W, trainX, trainY, learning_rate, num_epochs)\n",
    "    losses = []\n",
    "    for epoch in 1:num_epochs\n",
    "        y_hat = sigmoid_activation(compute_h(W, trainX))\n",
    "        # now that we have our predictions, we need to determine the\n",
    "        # 'error', which is the difference between our predictions and\n",
    "        # the true values\n",
    "        error = y_hat - trainY\n",
    "        append!(losses, 0.5 * sum(error .^ 2))\n",
    "        grad = compute_gradient(error, y_hat, trainX)\n",
    "        W -= learning_rate * grad\n",
    "\n",
    "        if epoch == 1 || epoch % 5 == 0\n",
    "            println(\"Epoch=$epoch; Loss=$(losses[end])\")\n",
    "        end\n",
    "    end\n",
    "    return W, losses\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7ccb65",
   "metadata": {},
   "source": [
    "#### Initialize our weight matrix and Train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb75494f",
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: Normal not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: Normal not defined\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ c:\\Users\\pc\\Downloads\\[Julia] Lab02 - GradientDescent.ipynb:1"
     ]
    }
   ],
   "source": [
    "W = rand(Normal(), (size(X_train)[2], 1))\n",
    "\n",
    "num_epochs=100\n",
    "learning_rate=0.1\n",
    "W, losses = train(W, X_train, y_train, learning_rate, num_epochs)\n",
    "plot(1:num_epochs, losses, legend=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ccd7e0",
   "metadata": {},
   "source": [
    "#### Evaluate result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4d28ad4",
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: W not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: W not defined\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ c:\\Users\\pc\\Downloads\\[Julia] Lab02 - GradientDescent.ipynb:1"
     ]
    }
   ],
   "source": [
    "preds = predict(W, X_test)\n",
    "acc = accuracy(preds, reshape(y_test, length(y_test), 1))\n",
    "p = precision(preds, reshape(y_test, length(y_test), 1))\n",
    "r = recall(preds, reshape(y_test, length(y_test), 1))\n",
    "f1 = 2*p*r/(p + r)\n",
    "print(\"acc: $acc, precision: $p, recall: $r, f1_score: $f1\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48cd45a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: preds not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: preds not defined\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ c:\\Users\\pc\\Downloads\\[Julia] Lab02 - GradientDescent.ipynb:2"
     ]
    }
   ],
   "source": [
    "# visualize the result of predictions\n",
    "df_test.y_hat = reshape(preds, (length(preds),))\n",
    "df_test |> @vlplot(\n",
    "    :point, \n",
    "    x=:x1, y=:x2, \n",
    "    color = :\"y_hat:n\",\n",
    "    width=400,height=400\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0526a8e9",
   "metadata": {},
   "source": [
    "**TODO: Study about accuracy, recall, precision, f1-score.**\n",
    "- Accuracy:\n",
    "- Recall:\n",
    "- Precision:\n",
    "- F1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffecb06b",
   "metadata": {},
   "source": [
    "**TODO: Try out different learning rates. Give me your observations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a142a93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "67e0cbc25fa4f5baaacba1240f401bc655b640f8e15cfc935dfee2e63491bdf4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
